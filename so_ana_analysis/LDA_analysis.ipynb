{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializsation\n",
    "\n",
    "import all relevant libraries and set options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "from IPython.display import Image, HTML, display\n",
    "import os\n",
    "import pyLDAvis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, date\n",
    "from typing import Dict, DefaultDict, Set, List\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "import yaml\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "desired_width =200\n",
    "pd.set_option('display.width', desired_width)\n",
    "np.set_printoptions(linewidth=desired_width)\n",
    "pd.set_option('display.max_columns', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import so_ana_doc_worker.so_ana_reporting as so_ana_reporting\n",
    "from so_ana_sqlalchemy_models.db_deps import prod_db_deps_container, dict_to_es_key\n",
    "import so_ana_management.management_utils as so_ana_mu\n",
    "from so_ana_util import data_access\n",
    "import so_ana_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get db connection data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deps = prod_db_deps_container()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load relevant analysis objects\n",
    "\n",
    "In order to access some job different from the latest one, browse the data frame \"all_jobs\" and replace flow_run_id by the desired value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_jobs = so_ana_reporting.ReportingDataAccess.load_all_job_data(deps.conn)\n",
    "flow_run_id = all_jobs.sort_values('started_at_timest', ascending=False).iloc[0, :]['flow_run_id']\n",
    "flow_run_id='a9507a44-d173-4de4-99f4-5e8a9c181b96'\n",
    "\n",
    "display(HTML(f'<h1>Last flow-run-id is: \"{flow_run_id}\"\"</h1>'))\n",
    "display(HTML('<hr width=\"85%\" align=\"left\">'))\n",
    "print()\n",
    "\n",
    "all_steps = so_ana_reporting.ReportingDataAccess.load_all_step_data_for_flow_run_id(  connection=deps.conn,\n",
    "                                                                                      flow_run_id=flow_run_id)\n",
    "\n",
    "flow_opts=all_jobs[all_jobs['flow_run_id']==flow_run_id]['flow_opts'].iloc[0]\n",
    "\n",
    "TAG_LABEL=flow_opts['topic']\n",
    "NR_TOPICS=flow_opts['ml_opts']['num_topics']\n",
    "BASE_OUTPUT=os.path.join(os.path.join(so_ana_util.PROJ_OUTP_PATH, flow_run_id), 'jupyter_LDA')\n",
    "try:\n",
    "    os.mkdir(BASE_OUTPUT)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "display(HTML(f'<h1>Steps for flow-run-id=\"{flow_run_id}\"</h1>'))\n",
    "display(all_steps.sort_index())\n",
    "print()\n",
    "display(HTML(f'<h1>Step results</h1>'))\n",
    "step_res_dict = {idx: item['result'] for idx, item in all_steps.iterrows()}\n",
    "print(yaml.dump(step_res_dict))\n",
    "display(HTML('<hr width=\"85%\" align=\"left\">'))\n",
    "display(HTML(f'<h1>All jobs available</h1>'))\n",
    "display(all_jobs)\n",
    "print()\n",
    "display(HTML('<h1>flow config</h1>'))\n",
    "print(yaml.dump(flow_opts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load relevant artefacts for flow run (LDA visualisaton model, gensim dictionary [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_data_access = so_ana_reporting.ReportingDataAccess(flow_run_id)\n",
    "step_result_dict = rep_data_access.load_job_step_data(sqla_session=deps.session)\n",
    "artefacts = rep_data_access.load_artefacts(deps.session)\n",
    "wc_report_obj = so_ana_reporting.WCReports( deps_obj=deps,\n",
    "                                            step='#3',\n",
    "                                            step_label=step_result_dict['#3'].step_label\n",
    "                                          )\n",
    "\n",
    "artefact_dict = {(item['step'], item['artefact_key']): item for item in artefacts}\n",
    "\n",
    "rep_wordcloud_artefact = artefact_dict[('#7', 'wordcloud')]\n",
    "rep_LDAvis_artefact = artefact_dict[('#7', 'LDAvis')] \n",
    "lda_model_artefact = artefact_dict[('#6', 'LDA_model_data')] \n",
    "dict_artefact = artefact_dict[('#5', 'dictionary')] \n",
    "\n",
    "lda_model = so_ana_reporting.LDA_from_artefact(lda_model_artefact)\n",
    "dictionary = so_ana_reporting.dictionary_from_artefact(dict_artefact)\n",
    "\n",
    "display(HTML('<h1>Artefacts</h1>'))\n",
    "print(yaml.dump(artefact_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(os.path.join(rep_wordcloud_artefact['artefact_value']['base_path'], \n",
    "                                   rep_wordcloud_artefact['artefact_value']['file_name']),\n",
    "     width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "    display(HTML('<h1>Wordclouds per topic</h1>'))\n",
    "    display(HTML('<hr width=\"85%\" align=\"left\">'))\n",
    "    for topic in range(NR_TOPICS):\n",
    "        display(HTML(f'<h2>Topic {topic+1}</h2>'))\n",
    "        fig= plt.figure(figsize=(20,10))\n",
    "        img = so_ana_reporting.WCReports.wc_for_topics_of_topic_model( lda_model_obj=lda_model,\n",
    "                                                                       topic_nr=topic,\n",
    "                                                                       file_name=None,\n",
    "                                                                       topn=100)\n",
    "        img\n",
    "        img.show()\n",
    "        display(HTML('<hr width=\"85%\" align=\"left\">'))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_key  =0\n",
    "    \n",
    "display(HTML('<h1>Wordclouds for first document</h1>'))\n",
    "print()\n",
    "\n",
    "wc_report = so_ana_reporting.WCReports(deps_obj=deps,\n",
    "                                       step='#5',\n",
    "                                       step_label=all_steps.loc['#5']['step_label'])\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "img = wc_report.wc_for_doc(ord_key=ord_key, file_name=None)\n",
    "img.show()\n",
    "    \n",
    "display(HTML('<hr width=\"85%\" align=\"left\">'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDAvis Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML('<h1>Mapping Gensim-Topics -> LDAVis-Topics</h1>'))\n",
    "for i, item in enumerate(sorted(artefact_dict[('#7', 'doc_topic_weights')]['artefact_value']['total_weight_sum'].items(), key=lambda x: x[1], reverse=True)):\n",
    "    print(f' {item[0]} (weight=\"{item[1]}\") -> {i+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML('<h1>LDAvis-Darstellung</h1>'))\n",
    "display(HTML(os.path.join(rep_LDAvis_artefact['artefact_value']['base_path'], \n",
    "                          rep_LDAvis_artefact['artefact_value']['file_name_html']\n",
    "                         )\n",
    "            )\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML('<h1>LÃ¤nge des Vokabulars</h1>'))\n",
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Post information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data frame with document meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_ext_post_meta_data(step_1_label, step_7_label, nr_topics):\n",
    "    qu = 'select * from so_ana_doc_worker.page_meta_info where step=%(step)s and step_label=%(step_label)s'\n",
    "\n",
    "    base = pd.read_sql(sql=qu, con=deps.conn, params={'step': '#1', 'step_label': step_1_label})\n",
    "    base.set_index('ord_key', inplace=True)\n",
    "\n",
    "    qu2 = 'select ord_key, topic_id, topic_weight from so_ana_analysis.topic_distribution where step=%(step)s and step_label=%(step_label)s and topic_id=%(topic_id)s'\n",
    "\n",
    "    for i in range(nr_topics):\n",
    "        pd_new = pd.read_sql(sql=qu2, con=deps.conn, params={'step': '#7', 'step_label': step_7_label, 'topic_id': i})\n",
    "        pd_new.set_index('ord_key', inplace=True)\n",
    "        base[f'w_topic_{i}'] = pd_new['topic_weight']\n",
    "\n",
    "    base.fillna(0, inplace=True)\n",
    "    return base\n",
    "\n",
    "\n",
    "def x_log_x(x, thrs=1E-10):\n",
    "    if x < thrs:\n",
    "        return -x*math.log2(thrs)\n",
    "    else:\n",
    "        return -x*math.log2(x)\n",
    "\n",
    "class DFEntropyMapper:\n",
    "    \n",
    "    def __init__(self, label_template, topic_number):\n",
    "        self.label_template = label_template\n",
    "        self.topic_number = topic_number\n",
    "        \n",
    "    def __call__(self, row):    \n",
    "        ret=0.0\n",
    "        for i in range(self.topic_number):\n",
    "            ret+= x_log_x(row[self.label_template.format(nr=i)])\n",
    "        return ret\n",
    "\n",
    "\n",
    "base = get_ext_post_meta_data(step_1_label=all_steps.loc['#1']['step_label'],\n",
    "                              step_7_label=all_steps.loc['#7']['step_label'],\n",
    "                              nr_topics=10)\n",
    "\n",
    "base['topic_entropy'] = base.apply(DFEntropyMapper('w_topic_{nr}', NR_TOPICS), axis=1)\n",
    "\n",
    "base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_label_list = ['votes', 'answers', 'answer_status', 'views']\n",
    "\n",
    "display(HTML('<h1>Histograms for quantitative post meta data</h1>'))\n",
    "\n",
    "for plt_data in hist_label_list:\n",
    "    rel_data = base[plt_data]\n",
    "    display(HTML(f'<h2>Histogram ({plt_data})'))\n",
    "    sns.histplot(rel_data, bins=100, cumulative=False, log_scale=False)\n",
    "    plt.show()\n",
    "    display(HTML(f'<h2>Histogram - cumulative ({plt_data})'))\n",
    "    sns.histplot(rel_data, bins=100, cumulative=True,  log_scale=False)\n",
    "    plt.show()\n",
    "    display(HTML(f'<h2>Histogram - topic weighted ({plt_data})'))\n",
    "    if not plt_data == 'answer_status':\n",
    "        for topic in range(NR_TOPICS):\n",
    "            rel_data_2 = base[f'w_topic_{topic}']*rel_data.astype('float')\n",
    "            display(HTML(f'<h3>Histogram topic [{plt_data}, topic={topic}]</h3>'))\n",
    "            sns.histplot(rel_data_2, bins=100, cumulative=False, log_scale=False)\n",
    "            plt.show()\n",
    "            display(HTML(f'<h3>Histogram topic - cumulative [{plt_data}, topic={topic}]</h3>'))\n",
    "            sns.histplot(rel_data_2, bins=100, cumulative=True, log_scale=False)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(base[[f'w_topic_{i}' for i in range(NR_TOPICS)]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tag-level information\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormTopMapper:\n",
    "    \n",
    "    def __init__(self, comp_series, topic_nr, max_topics):\n",
    "        self.comp_series = comp_series\n",
    "        self.topic_nr = topic_nr\n",
    "        self.max_topics = max_topics\n",
    "        \n",
    "    def __call__(self, row):\n",
    "        lbl=f'total_weight_topic_{self.topic_nr}'\n",
    "        norm = sum([row[f'total_weight_topic_{i}']/self.comp_series[f'total_weight_topic_{i}'] for i in range(self.max_topics)])\n",
    "        return row[lbl]/(self.comp_series[lbl]*norm)\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class BaseFrameData:\n",
    "    tags_data: pd.DataFrame\n",
    "    main_tag_data: pd.Series\n",
    "    main_tag_label: str\n",
    "    topic_nr: int\n",
    "    \n",
    "    \n",
    "def get_tags_frame_base(topic_nr, tag_label):  \n",
    "    label_cnt_dict=defaultdict(lambda: {'total_cnt': 0, **{f'total_weight_topic_{i}': 0 for i in range(topic_nr)}})\n",
    "\n",
    "    for idx, row in base.iterrows():\n",
    "        for lbl in row['tags']:\n",
    "            label_cnt_dict[lbl]['total_cnt']+=1\n",
    "            for i in range(topic_nr):\n",
    "                label_cnt_dict[lbl][f'total_weight_topic_{i}']+=row[f'w_topic_{i}']\n",
    "\n",
    "    for key, value in label_cnt_dict.items():\n",
    "        label_cnt_dict[key]['tag']=key\n",
    "\n",
    "    tags_frame=pd.DataFrame.from_records([res for res in label_cnt_dict.values()])\n",
    "    comp_weights = tags_frame[tags_frame['tag']==tag_label]\n",
    "    \n",
    "    return BaseFrameData(tags_data=tags_frame[tags_frame['tag']!=tag_label],\n",
    "                         main_tag_data=comp_weights,\n",
    "                         main_tag_label=tag_label,\n",
    "                         topic_nr=topic_nr\n",
    "                         )\n",
    "\n",
    "def extend_base_frame(base_data: BaseFrameData):\n",
    "    tags_frame=base_data.tags_data\n",
    "    for i in range(base_data.topic_nr):\n",
    "        tags_frame[f'normed_weight_{i}'] = tags_frame.apply(NormTopMapper(base_data.main_tag_data, i, base_data.topic_nr), \n",
    "                                                            axis=1)\n",
    "    lbl_lst=[f'normed_weight_{i}' for i in range(base_data.topic_nr)]\n",
    "    tags_frame['weight_entropy']=tags_frame.apply(DFEntropyMapper('normed_weight_{nr}', base_data.topic_nr), axis=1)\n",
    "    tags_frame['max_normed_topic_weight']=tags_frame[lbl_lst].max(axis=1)\n",
    "    tags_frame['dominant_normed_topic']=tags_frame[lbl_lst].apply(lambda row: row.argmax(), axis=1)\n",
    "    \n",
    "    return BaseFrameData(tags_data=tags_frame,\n",
    "                         main_tag_data=base_data.main_tag_data,\n",
    "                         main_tag_label=base_data.main_tag_label,\n",
    "                         topic_nr=base_data.topic_nr\n",
    "                         )\n",
    "\n",
    "\n",
    "    \n",
    "tag_level_data = get_tags_frame_base(topic_nr=NR_TOPICS, \n",
    "                                     tag_label=TAG_LABEL)\n",
    "\n",
    "tag_level_data = extend_base_frame(tag_level_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA_VAL=0.3\n",
    "display(HTML('<h1>Most relevant tags for topics</h1>'))\n",
    "\n",
    "def extend_by_scores(tag_level_data, lambda_val):\n",
    "    tags_frame=tag_level_data.tags_data\n",
    "    doc_cnt=tag_level_data.main_tag_data['total_cnt'].values[0]\n",
    "    for i in range(tag_level_data.topic_nr):\n",
    "        tags_frame[f'score_{i}']=lambda_val*tags_frame[f'normed_weight_{i}'] + (1-lambda_val)*tags_frame['total_cnt']/doc_cnt\n",
    "    return tags_frame\n",
    "\n",
    "tags_frame=extend_by_scores(tag_level_data, lambda_val=LAMBDA_VAL)\n",
    "doc_cnt=tag_level_data.main_tag_data['total_cnt'].values[0]\n",
    "tags_frame['total_count_rel'] = tags_frame['total_cnt']/doc_cnt\n",
    "\n",
    "\n",
    "for i in range(NR_TOPICS):\n",
    "    prt_data = tags_frame.sort_values(f'score_{i}', ascending=False).iloc[:10,:]\n",
    "    display(HTML(f'<h2>Topic: {i}</h2>'))\n",
    "    display(prt_data[['tag', 'weight_entropy', 'max_normed_topic_weight', 'dominant_normed_topic', 'total_cnt', f'score_{i}']])\n",
    "    \n",
    "    sns.scatterplot(data=prt_data, x=f'normed_weight_{i}', y='total_count_rel', hue='tag')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_labels=['total_cnt', 'weight_entropy', 'max_normed_topic_weight', 'dominant_normed_topic']\n",
    "hist_labels_templates=['total_weight_topic_{nr}', 'normed_weight_{nr}']\n",
    "tags_frame=tag_level_data.tags_data\n",
    "\n",
    "display(HTML('<h1>Histograms for tag level data data</h1>'))\n",
    "\n",
    "for plt_data in hist_labels:\n",
    "    rel_data = tags_frame[plt_data]\n",
    "    display(HTML(f'<h2>Histogram ({plt_data})'))\n",
    "    sns.histplot(rel_data, bins=100, cumulative=False, log_scale=False)\n",
    "    plt.show()\n",
    "    display(HTML(f'<h2>Histogram - cumulative ({plt_data})'))\n",
    "    sns.histplot(rel_data, bins=100, cumulative=True,  log_scale=False)\n",
    "    plt.show()\n",
    "    \n",
    "for topic_nr in range(NR_TOPICS):\n",
    "    display(HTML(f'<h2>Topic: ({topic_nr})'))\n",
    "    for plt_data_template in hist_labels_templates:\n",
    "        plt_data=plt_data_template.format(nr=topic_nr)\n",
    "        rel_data = tags_frame[plt_data]\n",
    "        display(HTML(f'<h2>Histogram (topic \"{topic_nr}\", {plt_data})'))\n",
    "        sns.histplot(rel_data, bins=100, cumulative=False, log_scale=False)\n",
    "        plt.show()\n",
    "        display(HTML(f'<h2>Histogram - cumulative (topic \"{topic_nr}\", {plt_data})'))\n",
    "        sns.histplot(rel_data, bins=100, cumulative=True,  log_scale=False)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_frame=tag_level_data.tags_data\n",
    "sns.pairplot(tags_frame[[f'normed_weight_{i}' for i in range(NR_TOPICS)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_frame=tag_level_data.tags_data\n",
    "tags_frame.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Link:\n",
    "    content: str\n",
    "    address: str\n",
    "        \n",
    "\n",
    "@dataclass\n",
    "class LinkAnalysisResult:\n",
    "    link_dict: DefaultDict[str, DefaultDict[str, float]]  = field(default_factory=lambda: defaultdict(lambda: defaultdict(lambda: 0.0)))\n",
    "    link_to_hit_docs: DefaultDict[str, Set] = field(default_factory=lambda: defaultdict(set))\n",
    "    link_to_linktype: Dict[str, str] = field(default_factory=dict)\n",
    "    link_data_frame: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "\n",
    "\n",
    "def extr_links(txt):\n",
    "    #soup = BeautifulSoup(f'<!DOCTYPE html><html><head></head><body>{txt}</body></html>', 'html.parser')\n",
    "    soup = BeautifulSoup(txt)\n",
    "    res = []\n",
    "    for item in soup.find_all('a'):\n",
    "        res.append(Link(item.get_text(), item.get('href', None)))\n",
    "    return res\n",
    "    \n",
    "    \n",
    "def make_list(item):\n",
    "    if isinstance(item, list):\n",
    "        ret=[]\n",
    "        for strval in item:\n",
    "            ret += extr_links(strval)\n",
    "        return ret\n",
    "    elif item is None or item == '':\n",
    "        return []\n",
    "    else:\n",
    "        return extr_links(item)\n",
    "\n",
    "    \n",
    "def check_endswith(what: str, lst: List[str]):\n",
    "    for item in lst:\n",
    "        if what.endswith(item):\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "def get_tuple(key, value, link_to_linktype, link_to_hit_docs, nr_topics):\n",
    "    lbl_lst = ['cnt_total', 'tot_quest_votes', 'tot_views', 'tot_answ_votes']\n",
    "    ret_lst = [key] + [value[lbl] for lbl in lbl_lst] + [value[f'w_topic_{i}'] for i in range(nr_topics)] + [link_to_linktype[key], link_to_hit_docs[key]]\n",
    "    return tuple(ret_lst)\n",
    "\n",
    "    \n",
    "def get_link_report(deps, base, step_3_label, nr_topics):\n",
    "    link_dict = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "    link_to_hit_docs = defaultdict(set)\n",
    "    link_to_linktype = dict()\n",
    "    i=0\n",
    "    IMG_END_LST = ['.gif', '.bmp', '.png', '.svg', '.jpg', '.tif', '.tiff', '.ico', '.jpeg']\n",
    "    doc_iterator = data_access.get_doc_iterator(connection=deps.conn,\n",
    "                                                d2es_obj=deps.d2es,\n",
    "                                                step_label=step_3_label,\n",
    "                                                format = 'all_#3',\n",
    "                                                ml_tags=None\n",
    "                    )\n",
    "    for item in doc_iterator:\n",
    "        i+=1   \n",
    "        links = make_list(item.question_links) + make_list(item.comment_link_lst)\n",
    "        for answ in item.answers:\n",
    "            new_entries = make_list(answ.answer_links) + make_list(answ.comment_link_lst)\n",
    "            for link in new_entries:\n",
    "                la = link.address.lower()\n",
    "                link_dict[la]['tot_answ_votes'] += answ.answer_vote or 0\n",
    "            links += new_entries\n",
    "        for link in links:\n",
    "            la = link.address.lower()\n",
    "            link_dict[la]['cnt_total'] += 1\n",
    "            link_dict[la]['tot_quest_votes'] += base.iloc[item.ord_key,:].loc['votes'] or 0\n",
    "            link_dict[la]['tot_views'] += base.iloc[item.ord_key,:].loc['views'] or 0  \n",
    "            for i in range(nr_topics):\n",
    "                link_dict[link.address.lower()][f'w_topic_{i}'] += base.iloc[item.ord_key,:].loc[f'w_topic_{i}']\n",
    "            link_to_hit_docs[la].add(item.post_id)\n",
    "            if check_endswith(la, IMG_END_LST):\n",
    "                link_to_linktype[la] = 'img'\n",
    "            elif la.startswith('/') or '/stackoverflow.com' in la:\n",
    "                link_to_linktype[la] = 'internal'\n",
    "            else:\n",
    "                link_to_linktype[la] = 'std'   \n",
    "                \n",
    "    lbl_lst = ['address', 'count', 'question_votes_total', 'views_total', 'answers_votes_total'] + [f'w_topic_{i}' for i in range(nr_topics)] + ['link type', 'ref_post_ids']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    link_data = pd.DataFrame.from_records([get_tuple(key, value, link_to_linktype, link_to_hit_docs, nr_topics=nr_topics) \n",
    "                                           for key, value \n",
    "                                           in link_dict.items()], columns=lbl_lst)\n",
    "    \n",
    "    for int_lbl in ['count', 'question_votes_total', 'views_total', 'answers_votes_total']:\n",
    "        link_data[int_lbl]=link_data[int_lbl].map(int)\n",
    "                \n",
    "    return LinkAnalysisResult(link_dict = link_dict,\n",
    "                              link_to_hit_docs = link_to_hit_docs,\n",
    "                              link_to_linktype = link_to_linktype,\n",
    "                              link_data_frame = link_data.sort_values('count', ascending=False)\n",
    "                              )\n",
    "\n",
    "    \n",
    "\n",
    "base = get_ext_post_meta_data(step_1_label=all_steps.loc['#1']['step_label'],\n",
    "                              step_7_label=all_steps.loc['#7']['step_label'],\n",
    "                              nr_topics=10)\n",
    "\n",
    "link_report = get_link_report(deps, base, step_3_label=all_steps.loc['#3']['step_label'], nr_topics=10)\n",
    "\n",
    "\n",
    "display(HTML('<h1>Anzahl der verschiedenen Links</h1>'))\n",
    "print(len(link_report.link_dict)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_iterator = data_access.get_doc_iterator(connection=deps.conn,\n",
    "                                            d2es_obj=deps.d2es,\n",
    "                                            step_label=all_steps.loc['#3']['step_label'],\n",
    "                                            format = 'all_#3',\n",
    "                                            ml_tags=None\n",
    ")\n",
    "\n",
    "link_dict = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "link_to_hit_docs = defaultdict(set)\n",
    "link_to_linktype = dict()\n",
    "\n",
    "@dataclass\n",
    "class Link:\n",
    "    content: str\n",
    "    address: str\n",
    "\n",
    "\n",
    "def extr_links(txt):\n",
    "    #soup = BeautifulSoup(f'<!DOCTYPE html><html><head></head><body>{txt}</body></html>', 'html.parser')\n",
    "    soup = BeautifulSoup(txt)\n",
    "    res = []\n",
    "    for item in soup.find_all('a'):\n",
    "        res.append(Link(item.get_text(), item.get('href', None)))\n",
    "    return res\n",
    "    \n",
    "def make_list(item):\n",
    "    if isinstance(item, list):\n",
    "        ret=[]\n",
    "        for strval in item:\n",
    "            ret += extr_links(strval)\n",
    "        return ret\n",
    "    elif item is None or item == '':\n",
    "        return []\n",
    "    else:\n",
    "        return extr_links(item)\n",
    "\n",
    "i=0\n",
    "for item in doc_iterator:\n",
    "    i+=1   \n",
    "    links = make_list(item.question_links) + make_list(item.comment_link_lst)\n",
    "    for answ in item.answers:\n",
    "        links += make_list(answ.answer_links) + make_list(answ.comment_link_lst)\n",
    "    for link in links:\n",
    "        la = link.address.lower()\n",
    "        link_dict[la]['cnt_total'] += 1\n",
    "        for i in range(10):\n",
    "            link_dict[la][f'w_topic_{i}'] += base.iloc[item.ord_key,:].loc[f'w_topic_{i}']\n",
    "        link_to_hit_docs[la].add(item.post_id)\n",
    "        if la.endswith('.gif') or la.endswith('.png') or la.endswith('.svg') or \\\n",
    "           la.endswith('.bmp') or la.endswith('.jpg') or la.endswith('.tif'):\n",
    "            link_to_linktype[la] = 'img'\n",
    "        elif la.startswith('/') or '/stackoverflow.com' in la:\n",
    "            link_to_linktype[la] = 'internal'\n",
    "        else:\n",
    "            link_to_linktype[la] = 'std'          \n",
    "\n",
    "\n",
    "for key in link_dict.keys():\n",
    "    link_dict[key]['link_target']=key\n",
    "    link_dict[key]['link_type']=link_to_linktype[key]\n",
    "    link_dict[key]['doc_hits']=link_to_hit_docs[key]\n",
    "\n",
    "    \n",
    "link_data = pd.DataFrame.from_records([value\n",
    "                                       for value \n",
    "                                       in link_dict.values()])\n",
    "\n",
    "display(HTML('<h1>Link data</h1>'))\n",
    "display(HTML('<h2>Number of links:</h2>'))\n",
    "print(len(link_dict))     \n",
    "display(HTML('<h2>Top link list:</h2>'))\n",
    "display(link_data.sort_values('cnt_total', ascending=False).head())\n",
    "display(HTML('<h2>Without images and internal links:</h2>'))\n",
    "display(link_data[link_data['link_type']=='std'].sort_values('cnt_total', ascending=False).head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export in csv und xls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise NotImplementedError()\n",
    "base.to_csv(os.path.join(BASE_OUTPUT, 'meta_data.csv'), sep=';')\n",
    "link_data.to_csv(os.path.join(BASE_OUTPUT, 'link_infos.csv'), sep=';')\n",
    "base.to_excel(os.path.join(BASE_OUTPUT, 'meta_data.xlsx'))\n",
    "link_data.to_excel(os.path.join(BASE_OUTPUT, 'link_infos.xlsx'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Verteilung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_data = link_data[link_data['link_type']=='std']['cnt_total']\n",
    "display(HTML('<h1>Histogram</h1>'))\n",
    "sns.histplot(rel_data, bins=100, cumulative=False, log_scale=False)\n",
    "plt.show()\n",
    "display(HTML('<h1>Histogram - cumulative</h1>'))\n",
    "sns.histplot(rel_data, bins=100, cumulative=True,  log_scale=True)\n",
    "plt.show()\n",
    "print()\n",
    "print()\n",
    "display(HTML('<h1>Histogram of Number weighted by topic weights</h1>'))\n",
    "for topic in range(10):\n",
    "    rel_data_2 = link_data[link_data['link_type']=='std'][f'w_topic_{topic}']\n",
    "    display(HTML(f'<h2>Histogram topic [{topic}]</h2>'))\n",
    "    sns.histplot(rel_data_2, bins=100, cumulative=False, log_scale=False)\n",
    "    plt.show()\n",
    "    display(HTML(f'<h2>Histogram topic [{topic}] - cumulative</h2>'))\n",
    "    sns.histplot(rel_data_2, bins=100, cumulative=True, log_scale=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temportal post Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_filtered = base[base['asked_date'].map(lambda x: isinstance(x, type(date.today())))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_nr = 40\n",
    "\n",
    "time_delta = (base_filtered['asked_date'].max()-base_filtered['asked_date'].min())/bin_nr\n",
    "\n",
    "bins = [base_filtered['asked_date'].min()+i*time_delta for i in range(bin_nr+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bin(value):\n",
    "    for i in range(bin_nr):\n",
    "        if bins[i] < value and value<= bins[i+1]:\n",
    "            return f'[{i+1}] {bins[i]}-{bins[i+1]}'\n",
    "    else:\n",
    "        return f'[{0}] <={bins[0]}'\n",
    "    \n",
    "base_filtered['date_nr_bins'] = base_filtered['asked_date'].map(get_bin)\n",
    "\n",
    "gr_data = base_filtered.groupby('date_nr_bins')['step'].count()\n",
    "gr_data_weights = base_filtered.groupby('date_nr_bins')[[f'w_topic_{topic}' for topic in range(10)]].sum()\n",
    "topics_wighted_by_total_weight = gr_data_weights.div(gr_data_weights.sum(axis=1),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(f'<h1>number of post for time intervals (len: {time_delta.days} days)</h1>'))\n",
    "gr_data.plot(kind='bar', figsize=(20, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML('<h1>sum of number of posts weighted by topic weights</h1>'))\n",
    "gr_data_weights.plot(kind='bar', stacked=True, figsize=(20, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML('<h1>sum of number of posts weighted by topic weights / normalized</h1>'))\n",
    "topics_wighted_by_total_weight.plot(kind='bar', stacked=True, figsize=(20, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
