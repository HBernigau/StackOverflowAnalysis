{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from IPython.display import Image, HTML, display, clear_output\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from typing import Tuple, Any\n",
    "import itertools\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime\n",
    "import scipy\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from functools import lru_cache\n",
    "import math\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import so_ana_doc_worker.so_ana_reporting as so_ana_reporting\n",
    "from sqlalchemy_models.db_deps import prod_db_deps_container, dict_to_es_key\n",
    "import so_ana_management.management_utils as so_ana_mu\n",
    "from so_ana_util import data_access\n",
    "import so_ana_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NR_ITERATE = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define relevant helper objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ProgressBar:\n",
    "    heading: str\n",
    "    progr_str: str = field(default='Progress: {ratio:.4f}%')\n",
    "    duration_prfx_str: str = field(default='Duration: ')\n",
    "    width_total_in_px: int = field(default=500)\n",
    "    height_total_in_px: int = field(default=30)\n",
    "    bg_color: str = field(default='DodgerBlue')\n",
    "    txt_color: str = field(default='White')\n",
    "        \n",
    "    def take_time_snapshot(self):\n",
    "        self.curr_time = datetime.now()\n",
    "    \n",
    "    def update_progress(self, progress):\n",
    "        if progress == 0:\n",
    "            self.take_time_snapshot()\n",
    "        if isinstance(progress, int):\n",
    "            progress = float(progress)\n",
    "        if not isinstance(progress, float):\n",
    "            progress = 0\n",
    "        if progress < 0:\n",
    "            progress = 0\n",
    "        if progress >= 1:\n",
    "            progress = 1\n",
    "            \n",
    "        clear_output(wait = True)\n",
    "\n",
    "        width = int(progress * (self.width_total_in_px - 2))\n",
    "        ratio_txt = self.progr_str.format(ratio=100*progress)\n",
    "        time_delta_str = self.duration_prfx_str + str(datetime.now()-self.curr_time)\n",
    "        html_txt = f'<h2>{self.heading}</h2>'\n",
    "        html_txt += f'<div>{ratio_txt}</div>'\n",
    "        html_txt += f'<div style=\"width:{self.width_total_in_px}px;height:{self.height_total_in_px}px;border:1px solid #000;\">'\n",
    "        html_txt += f'<div style=\"width:{width}px;height:{self.height_total_in_px-2}px;border:1px solid #000;background-color:{self.bg_color};color:{self.txt_color}\">'\n",
    "        html_txt += '</div></div>'\n",
    "        html_txt += f'<div>{time_delta_str}</div>'\n",
    "        display(HTML(html_txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get db connection data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deps = prod_db_deps_container()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_jobs = so_ana_reporting.ReportingDataAccess.load_all_job_data(deps.conn)\n",
    "flow_run_id = all_jobs.sort_values('started_at_timest', ascending=False).iloc[0, :]['flow_run_id']\n",
    "\n",
    "display(HTML(f'<h1>Last flow-run-id is: \"{flow_run_id}\"\"</h1>'))\n",
    "display(HTML('<hr width=\"85%\" align=\"left\">'))\n",
    "print()\n",
    "\n",
    "all_steps = so_ana_reporting.ReportingDataAccess.load_all_step_data_for_flow_run_id(  connection=deps.conn,\n",
    "                                                                                      flow_run_id=flow_run_id)\n",
    "\n",
    "flow_opts=all_jobs[all_jobs['flow_run_id']==flow_run_id]['flow_opts'].iloc[0]\n",
    "\n",
    "ROOT_PATH =os.path.join(os.path.join(so_ana_util.PROJ_OUTP_PATH, flow_run_id), 'jupyter_network')\n",
    "os.mkdir(ROOT_PATH )\n",
    "EXP_PATH_USER_GRAPH = os.path.join(ROOT_PATH, 'user_graph.xlsx')\n",
    "EXP_PATH_DOC_GRAPH = os.path.join(ROOT_PATH, 'document_graph.xlsx')\n",
    "EXP_PATH_DISTR =  os.path.join(ROOT_PATH, 'graph_distribution_@GRAPHNAME@.xlsx')\n",
    "TAG_LABEL=flow_opts['topic']\n",
    "NR_TOPICS=flow_opts['ml_opts']['num_topics']\n",
    "\n",
    "display(HTML(f'<h1>Steps for flow-run-id=\"{flow_run_id}\"</h1>'))\n",
    "display(all_steps.sort_index())\n",
    "\n",
    "display(HTML('<hr width=\"85%\" align=\"left\">'))\n",
    "print()\n",
    "display(all_jobs)\n",
    "print()\n",
    "display(HTML('<h1>flow config</h1>'))\n",
    "print(yaml.dump(flow_opts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define relevant graph object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DocInfo:\n",
    "    post_id: int\n",
    "        \n",
    "@dataclass(frozen=True)\n",
    "class UserInfo:\n",
    "    user_url: str\n",
    "               \n",
    "        \n",
    "def multi_graph_2_weighted_graph(gr):\n",
    "    if gr.is_directed():\n",
    "        new_gr = nx.DiGraph(name='Reduced' + gr.graph['name'])\n",
    "    else:\n",
    "        new_gr = nx.Graph(name='Reduced' + gr.graph['name'])\n",
    "    \n",
    "    for node in gr.nodes:\n",
    "        new_gr.add_node(node)\n",
    "    \n",
    "    for edge in gr.edges:\n",
    "        if new_gr.has_edge(edge[0], edge[1]):\n",
    "            new_gr[edge[0]][edge[1]]['weight'] += 1\n",
    "        else:\n",
    "            new_gr.add_edge(edge[0], edge[1], weight=1)\n",
    "            \n",
    "    return new_gr\n",
    "\n",
    "def reverse(gr):\n",
    "    res = gr.reverse(copy=True)\n",
    "    res.graph['name'] = 'Reversed' + res.graph['name']\n",
    "    return res\n",
    "\n",
    "    \n",
    "class DocUserGraph:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.G = nx.MultiDiGraph()\n",
    "        \n",
    "    def add_document(self,post_id, user_url, topic_tuple, answ_user_url_lst):\n",
    "        doc_node = DocInfo(post_id=post_id)\n",
    "        user_node = UserInfo(user_url=user_url)\n",
    "        self.G.add_node(doc_node, topic_weights=np.array(topic_tuple))\n",
    "        self.G.add_node(user_node)\n",
    "        self.G.add_edge(doc_node, user_node)\n",
    "        for item in answ_user_url_lst:\n",
    "            answ_user_node = UserInfo(user_url=item)\n",
    "            self.G.add_node(answ_user_node)\n",
    "            self.G.add_edge(answ_user_node, doc_node)\n",
    "    \n",
    "    def _get_user_weight(self, user_info):\n",
    "        weight = None\n",
    "        for ng in  itertools.chain(self.G.successors(user_info), self.G.predecessors(user_info)):\n",
    "            if weight is None:\n",
    "                weight = np.copy(self.G.nodes[ng]['topic_weights'])\n",
    "            else:\n",
    "                weight += self.G.nodes[ng]['topic_weights']         \n",
    "                \n",
    "        return weight / np.sum(weight)\n",
    "    \n",
    "    @property        \n",
    "    def user_response_graph(self):\n",
    "        new_gr = nx.MultiDiGraph(name='UserResponseGraph')\n",
    "        for node in self.G:\n",
    "            if isinstance(node, UserInfo):\n",
    "                weight = self._get_user_weight(node)\n",
    "                new_gr.add_node(node, topic_weights=weight)\n",
    "                for ng1 in self.G.successors(node):\n",
    "                    for ng2 in self.G.successors(ng1):\n",
    "                        if not ng2 == UserInfo(None):\n",
    "                            weight2 = self._get_user_weight(ng2)\n",
    "                            new_gr.add_node(ng2, topic_weights=weight2)\n",
    "                            new_gr.add_edge(node, ng2)             \n",
    "        return new_gr\n",
    "    \n",
    "    @property \n",
    "    def document_contr_graph(self):\n",
    "        new_gr = nx.MultiGraph(name='DocumentUserSharedGraph')\n",
    "        node_set = set()\n",
    "        for node in self.G:\n",
    "            if isinstance(node, DocInfo):\n",
    "                weight = self.G.nodes[node]['topic_weights']\n",
    "                new_gr.add_node(node, topic_weights=weight)\n",
    "                node_set.add(node)\n",
    "                for ng1 in itertools.chain(self.G.successors(node), self.G.predecessors(node)):\n",
    "                    for ng2 in itertools.chain(self.G.successors(ng1), self.G.predecessors(ng1)):\n",
    "                        if not ng2 in node_set:\n",
    "                            weight2 = self.G.nodes[ng2]['topic_weights']\n",
    "                            new_gr.add_node(ng2, weight_vector=weight2)\n",
    "                            new_gr.add_edge(node, ng2)\n",
    "        return new_gr\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct basic graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get document iterator for relevant step\n",
    "doc_iterator = data_access.get_doc_iterator(connection=deps.conn,\n",
    "                                            d2es_obj=deps.d2es,\n",
    "                                            step_label=all_steps.loc['#3']['step_label'],\n",
    "                                            format = 'all_#3',\n",
    "                                            ml_tags=None\n",
    "                    )\n",
    "\n",
    "nr_docs = len(doc_iterator)\n",
    "\n",
    "# create relevant graph\n",
    "graph = DocUserGraph()\n",
    "\n",
    "query = 'select user_url from so_ana_doc_worker.page_meta_info ' \\\n",
    "        'where step=%(step)s and step_label=%(step_label)s and post_id=%(post_id)s'\n",
    "\n",
    "query2 = 'select topic_id, topic_weight from so_ana_analysis.topic_distribution '\\\n",
    "         'where step=%(step)s and step_label=%(step_label)s and post_id=%(post_id)s order by topic_id asc'\n",
    "\n",
    "progr_bar = ProgressBar('Constructing document-user graph...')\n",
    "\n",
    "for i, qu in enumerate(doc_iterator):\n",
    "    if i % 10 == 0:\n",
    "        progr_bar.update_progress(i/nr_docs)\n",
    "    res = deps.conn.execute(query,   {'step': '#1',\n",
    "                                      'step_label': all_steps.loc['#1']['step_label'],\n",
    "                                      'post_id': qu.post_id\n",
    "                             }).fetchone()[0]\n",
    "    weight_res = {item[0]: item[1] for item in  deps.conn.execute(query2,   { 'step': '#7',\n",
    "                                                                              'step_label': all_steps.loc['#7']['step_label'],\n",
    "                                                                              'post_id': qu.post_id\n",
    "                                                                     }\n",
    "                                                          )\n",
    "                 }\n",
    "    graph.add_document(post_id=qu.post_id, user_url=res,  topic_tuple=tuple([weight_res.get(i, 0) for i in range(NR_TOPICS)]), answ_user_url_lst=[answ.user_url for answ in qu.answers])\n",
    "\n",
    "progr_bar.update_progress(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load relevant data into pandas data frames \"user_response_data\" and \"doc_response_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_iterator(user_response_graph):\n",
    "    for node in user_response_graph.nodes():\n",
    "        wv = user_response_graph.nodes[node]['topic_weights']\n",
    "        in_deg = user_response_graph.in_degree(node)\n",
    "        out_deg = user_response_graph.out_degree(node)\n",
    "        yield [node.user_url, in_deg, out_deg, out_deg-in_deg] + wv.tolist()\n",
    "\n",
    "        \n",
    "def doc_iterator(doc_graph):\n",
    "        for node in doc_graph.nodes():\n",
    "            wv = doc_graph.nodes[node]['topic_weights']\n",
    "            yield [node.post_id, doc_graph.degree(node)] + wv.tolist()\n",
    "\n",
    "    \n",
    "    \n",
    "user_response_graph = graph.user_response_graph        \n",
    "user_response_data = pd.DataFrame.from_records(user_iterator(user_response_graph), columns=['user', 'in degree', 'out degree', 'degree difference'] + [f'weight {i}' for i in range(NR_TOPICS)])\n",
    "\n",
    "display(HTML('<h1>User response graph</h1>'))\n",
    "display(user_response_data.head())\n",
    "\n",
    "doc_graph = graph.document_contr_graph\n",
    "doc_response_data = pd.DataFrame.from_records(doc_iterator(doc_graph), columns=['document', 'degree'] + [f'weight {i}' for i in range(NR_TOPICS)])\n",
    "\n",
    "display(HTML('<h1>Doc contribution graph</h1>'))\n",
    "display(doc_response_data.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment node level graph data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedMapper:\n",
    "    \n",
    "    def __init__(self, DC):\n",
    "        self.DC = DC\n",
    "        self.mapping_dict = {}\n",
    "       \n",
    "    def __call__(self, x):\n",
    "        return self.mapping_dict[self.DC(x)]     \n",
    "\n",
    "class HashedReturn:\n",
    "    \n",
    "    def __init__(self, rel_func):\n",
    "        self.rel_func = rel_func\n",
    "        self._res = None\n",
    "        self.key_hsh = ''\n",
    "        self.val_hsh = ''\n",
    "        self.gr_name = ''\n",
    "        \n",
    "    def __call__(self, gr, **kwargs):\n",
    "        recalc = (self._res is None) or (self.key_hsh != hash(tuple(kwargs.keys()))) or (self.val_hsh != hash(tuple(kwargs.values()))) or (self.gr_name != gr.graph['name'])\n",
    "        self.key_hsh =  hash(tuple(kwargs.keys()))\n",
    "        self.val_hsh = hash(tuple(kwargs.values()))\n",
    "        self.gr_name = gr.graph['name']\n",
    "        if recalc:\n",
    "            self._res = self.rel_func(gr, **kwargs)\n",
    "        return self._res\n",
    "    \n",
    "def get_projection(f, pos):\n",
    "    def _inner(*args, **kwargs):\n",
    "        return f(*args, **kwargs)[pos]\n",
    "    return _inner         \n",
    "        \n",
    "\n",
    "def augment_centralities(df, graph):\n",
    "    red_graph =  multi_graph_2_weighted_graph(graph)\n",
    "    gr_name = graph.graph['name']\n",
    "    res_dict = {}\n",
    "    if gr_name == 'UserResponseGraph':\n",
    "        aug_mapper = AugmentedMapper(UserInfo)\n",
    "        rel_label = 'user'\n",
    "        hsh_hits_func = HashedReturn(nx.algorithms.link_analysis.hits_alg.hits)\n",
    "        \n",
    "        lbl_lst = [ ('centrality_in_degree',  nx.algorithms.centrality.in_degree_centrality, {}),\n",
    "                    ('centrality_out_degree',  nx.algorithms.centrality.out_degree_centrality, {}),\n",
    "                    ('centrality_eigenvector',  nx.algorithms.centrality.eigenvector_centrality, dict(weight='weight', tol=1.0e-6)),\n",
    "                    ('centrality_page_rank',  nx.algorithms.link_analysis.pagerank_alg.pagerank, dict(alpha=0.85, weight='weight', tol=1.0e-6)),\n",
    "                    ('centrality_hits_hubs',  get_projection(hsh_hits_func, 0), {}),\n",
    "                    ('centrality_hits_authorities',  get_projection(hsh_hits_func, 1), {}),\n",
    "                    ('centrality_harmonic',  nx.algorithms.centrality.harmonic_centrality, {})\n",
    "                  ]\n",
    "    elif gr_name == 'DocumentUserSharedGraph':\n",
    "        aug_mapper = AugmentedMapper(DocInfo)\n",
    "        rel_label = 'document'\n",
    "        lbl_lst = [ ('centrality_degree',  nx.algorithms.centrality.degree_centrality, {}),\n",
    "                  #  ('centrality_eigenvector',  nx.algorithms.centrality.eigenvector_centrality, dict(weight='weight', tol=1.0e-6)),\n",
    "                    ('centrality_page_rank',  nx.algorithms.link_analysis.pagerank_alg.pagerank, dict(alpha=0.85, weight='weight', tol=1.0e-6)),\n",
    "                    ('centrality_harmonic',  nx.algorithms.centrality.harmonic_centrality, {})\n",
    "                  ]\n",
    "    elif gr_name == 'ReversedUserResponseGraph':\n",
    "        lbl_lst = [ ('centrality_reversed_page_rank',  nx.algorithms.link_analysis.pagerank_alg.pagerank, dict(alpha=0.85, weight='weight', tol=1.0e-6))]\n",
    "        rel_label = 'user'\n",
    "        aug_mapper = AugmentedMapper(UserInfo)\n",
    "        \n",
    "    for label, alg, kwargs in lbl_lst:\n",
    "        print(f'calculating \"{label}\" for graph \"{gr_name}\"...')\n",
    "        res_dict[label] = alg(red_graph, **kwargs)\n",
    "\n",
    "    for label, rel_dict in res_dict.items():\n",
    "        print(f'starting {label} for graph {gr_name}')\n",
    "        aug_mapper.mapping_dict = rel_dict\n",
    "        df[label] = df[rel_label].map(aug_mapper)\n",
    "    return df\n",
    "\n",
    "for rel_data, rel_gr in [(doc_response_data, doc_graph), (user_response_data, user_response_graph), (user_response_data, reverse(user_response_graph))]:\n",
    "    gr_name = rel_gr.graph['name']\n",
    "    display(HTML(f'<h2>{gr_name}</h2>'))\n",
    "    rel_data = augment_centralities(rel_data, rel_gr)  \n",
    "    display(rel_data.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Plot histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.despine()\n",
    "\n",
    "fig1, f1_axes = plt.subplots(ncols=2, nrows=3, constrained_layout=True, figsize=(15, 15))\n",
    "plt.suptitle('Degree distributions', fontsize=24)\n",
    "\n",
    "for i, (rel_data, lbl, gr_name) in enumerate(\n",
    "                                                [(doc_response_data, 'degree', doc_graph.graph['name']), \n",
    "                                                 (user_response_data, 'in degree', user_response_graph.graph['name']), \n",
    "                                                 (user_response_data, 'out degree', user_response_graph.graph['name'])\n",
    "                                                ]\n",
    "                                            ):\n",
    "    rel_txt = f'\"{lbl}\" of graph \"{gr_name}\"'\n",
    "    display(HTML(f'<h2>Plotting histogram for distribution of ' + rel_txt + '</h2>'))\n",
    "    for j, bool_density in enumerate([True, False]):\n",
    "        rel_data[[lbl]].hist(bins=50, density=bool_density, ax = f1_axes[i, j])\n",
    "        f1_axes[i, j].set_title(rel_txt + (' (density)' if bool_density else ' (total count)'))\n",
    "\n",
    "    print('Kennzahlen')\n",
    "    display(rel_data[[lbl]].describe(percentiles=[.001, 0.01, 0.05, 0.1,.2,.3,.4,.5,.6,.7,.8,.9, 0.95, .99, 0.999]))\n",
    "    descr = scipy.stats.describe(rel_data[lbl])\n",
    "    for attri in ['skewness', 'kurtosis']:\n",
    "        print(f'{attri}: {getattr(descr, attri)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export node level graph data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_response_data.to_excel(EXP_PATH_USER_GRAPH)\n",
    "doc_response_data.to_excel(EXP_PATH_DOC_GRAPH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate key metrics for relevant graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(f'<h1>Key metrics of graphs</h1>'))\n",
    "for gr in (graph.G, user_response_graph, doc_graph, multi_graph_2_weighted_graph(user_response_graph), multi_graph_2_weighted_graph(doc_graph)):\n",
    "    gr_name = gr.graph.get('name', 'FullDocumentUserGraph')\n",
    "    display(HTML(f'<h2>{gr_name}</h2>'))\n",
    "    nr_nodes = gr.number_of_nodes()\n",
    "    nr_edges = gr.number_of_edges()\n",
    "    print('number of nodes: ', nr_nodes)\n",
    "    print('number of edges: ', nr_edges)\n",
    "    print('average degree: ', nr_edges/nr_nodes)\n",
    "    if gr_name in ('UserResponseGraph', 'ReducedUserResponseGraph', 'FullDocumentUserGraph'):\n",
    "        print('Strongly connected components: ', nx.number_strongly_connected_components(gr))\n",
    "        print('Weakly connected components: ', nx.number_weakly_connected_components(gr))\n",
    "    elif gr_name in ('DocumentUserSharedGraph', 'ReducedDocumentUserSharedGraph'):\n",
    "        print('Connected components: ', nx.number_connected_components(gr))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate average cosine similarity and randomized empirical distribution of cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosSimCalculator:\n",
    "    \n",
    "    def __init__(self, graph):\n",
    "        self.graph = graph\n",
    "        nr_nodes = graph.number_of_nodes()\n",
    "        for node in graph.nodes():\n",
    "            nr_weights = graph.nodes[node]['topic_weights'].shape[0]\n",
    "            break\n",
    "        self.weight_array = np.zeros(shape=(nr_nodes, nr_weights), dtype=float)\n",
    "        self.node2posdict = {}\n",
    "        for i, node in enumerate(graph.nodes()):\n",
    "            self.node2posdict[node] = i\n",
    "            for j in range(nr_weights):\n",
    "                self.weight_array[i, j] = graph.nodes[node]['topic_weights'][j]\n",
    "        \n",
    "        self.edge_list = []\n",
    "        for edge in graph.edges():\n",
    "            self.edge_list.append((self.node2posdict[edge[0]], self.node2posdict[edge[1]]))\n",
    "        self.nodepair_2_value_dict = {}\n",
    "        \n",
    "    @property\n",
    "    def nr_edges(self):\n",
    "        return len(self.edge_list)\n",
    "    \n",
    "    @property\n",
    "    def nr_nodes(self):\n",
    "        return self.weight_array.shape[0]\n",
    "    \n",
    "    @property\n",
    "    def nr_weights(self):\n",
    "        return self.weight_array.shape[1]\n",
    "    \n",
    "    def get_cos_sim(self, i, j):\n",
    "        a = self.weight_array[i]\n",
    "        b = self.weight_array[j]\n",
    "        return np.dot(a, b) / (np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    \n",
    "    def __call__(self, permut_array = None):\n",
    "        if permut_array is None:\n",
    "            permut_array = np.arange(self.nr_nodes)\n",
    "        sum_of_cos = 0.0\n",
    "        for e1, e2 in self.edge_list:\n",
    "            i = permut_array[e1]\n",
    "            j = permut_array[e2]\n",
    "            if i<=j:\n",
    "                key = (i, j)\n",
    "            else:\n",
    "                key = (j, i)\n",
    "            new_cos_sim = self.nodepair_2_value_dict.get(key, 0)\n",
    "            if new_cos_sim == 0:\n",
    "                new_cos_sim = self.get_cos_sim(i, j)\n",
    "                self.nodepair_2_value_dict[key] = new_cos_sim\n",
    "            sum_of_cos += new_cos_sim\n",
    "        \n",
    "        return sum_of_cos / self.nr_edges\n",
    "        \n",
    "@dataclass\n",
    "class SimilarityCalculationResult:\n",
    "    graph: Any\n",
    "    NR_ITERATE: int = 100\n",
    "    res_avg_cos_sim: float = 0.0\n",
    "    res_emp_cos_sim_dist: pd.DataFrame = None\n",
    "        \n",
    "    def calculate_emp_distr(self, progr_bar):\n",
    "        cos_sim_calc = CosSimCalculator(self.graph)\n",
    "        self.res_avg_cos_sim = cos_sim_calc()\n",
    "        nr_nodes = cos_sim_calc.nr_nodes\n",
    "        \n",
    "        res_lst = []\n",
    "        for i in range(self.NR_ITERATE):\n",
    "            rnd_perm = np.random.permutation(nr_nodes)\n",
    "            progr_bar.update_progress(i/self.NR_ITERATE)\n",
    "            res_lst.append(cos_sim_calc(rnd_perm))\n",
    "        progr_bar.update_progress(1.0)\n",
    "        self.res_emp_cos_sim_dist = pd.DataFrame(res_lst, columns=['random cosine similarity'])\n",
    "         \n",
    "#generate calculation results\n",
    "cos_sim_res_lst = []\n",
    "for gr in [user_response_graph, doc_graph]: \n",
    "    new_res = SimilarityCalculationResult(graph = gr, NR_ITERATE = NR_ITERATE)\n",
    "    gr_name = gr.graph['name']\n",
    "    progr_bar = ProgressBar(f'Calculating results for graph \"{gr_name}\"...')\n",
    "    new_res.calculate_emp_distr(progr_bar)\n",
    "    cos_sim_res_lst.append(new_res)\n",
    "\n",
    "#output most average cosine similarity and its quantile within the emirical distribution\n",
    "display(HTML('<h1>Results</h1>'))\n",
    "for res in cos_sim_res_lst:\n",
    "    gr_name = res.graph.graph['name']\n",
    "    display(HTML(f'<h2>Graph \"{gr_name}\"</h2>'))\n",
    "    print('Average cosine similarity of connected documents: ', res.res_avg_cos_sim)\n",
    "    emp_dist = ECDF(res.res_emp_cos_sim_dist['random cosine similarity'])\n",
    "    print('Average cosine similarity equates to the following quantile of empirical distribution: ', emp_dist(res.res_avg_cos_sim))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eport Graph data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in cos_sim_res_lst:\n",
    "    res.res_emp_cos_sim_dist.to_excel(EXP_PATH_DISTR.replace('@GRAPHNAME@', res.graph.graph['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML('<h1>Key metrics of empirical cosine similarity distribution</h1>'))\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.despine()\n",
    "\n",
    "fig2, f2_axes = plt.subplots(ncols=2, nrows=len(cos_sim_res_lst), constrained_layout=True, figsize=(15, 15), squeeze=False)\n",
    "plt.suptitle('Empirical distributions of average cosine similarity', fontsize=24)\n",
    "\n",
    "for i, res in enumerate(cos_sim_res_lst):\n",
    "    gr_name = res.graph.graph['name']\n",
    "    display(HTML(f'<h2>Graph \"{gr_name}\"</h2>'))\n",
    "    for j, density_bool in enumerate([True, False]):\n",
    "        res.res_emp_cos_sim_dist.hist(bins=50, ax = f2_axes[i, j])\n",
    "        f2_axes[i, j].set_title(f'Graph \"{gr_name}\" ' + ('(density)' if density_bool else '(total count)'))\n",
    "\n",
    "    print(f'Average cosine similarity: ',  res.res_avg_cos_sim)\n",
    "    emp_dist = ECDF(res.res_emp_cos_sim_dist['random cosine similarity'])\n",
    "    e_qu = emp_dist(res.res_avg_cos_sim)\n",
    "    print('Corresponding quantile in empirical distribution: ', e_qu)\n",
    "    if e_qu > 0.975 or e_qu < 0.025:\n",
    "        txt = 'highly significant'\n",
    "    elif e_qu > 0.95 or e_qu < 0.05:\n",
    "        txt = 'significant'\n",
    "    else:\n",
    "        txt = 'not significant'\n",
    "    print('     -> Assortativity result is ' + txt)\n",
    "    descr_pd = res.res_emp_cos_sim_dist.describe(percentiles=[.001, 0.01, 0.05, 0.1,.2,.3,.4,.5,.6,.7,.8,.9, 0.95, .99, 0.999])\n",
    "    d_for_VA =  (res.res_avg_cos_sim - descr_pd.loc['mean', 'random cosine similarity'])/descr_pd.loc['std', 'random cosine similarity']\n",
    "    alpha_for_VA = d_for_VA/math.sqrt(d_for_VA*d_for_VA + 1.0)\n",
    "    print('VA-Index: ', alpha_for_VA)\n",
    "    display(descr_pd)\n",
    "    descr = scipy.stats.describe(res.res_emp_cos_sim_dist['random cosine similarity'])\n",
    "    for attri in ['skewness', 'kurtosis']:\n",
    "        print(f'{attri}: {getattr(descr, attri)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
